{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Translation\n",
    "\n",
    "In this project, you’re going to take a peek into the realm of neural network machine translation. You’ll be training a sequence to sequence model on a dataset of English and French sentences that can translate new sentences from English to French.\n",
    "\n",
    "## Get the Data\n",
    "\n",
    "Since translating the whole language of English to French will take lots of time to train, we have provided you with a small portion of the English corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "source_path = 'data/small_vocab_en'\n",
    "target_path = 'data/small_vocab_fr'\n",
    "source_text = helper.load_data(source_path)\n",
    "target_text = helper.load_data(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "Play around with view_sentence_range to view different parts of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 227\n",
      "Number of sentences: 137861\n",
      "Average number of words in a sentence: 13.225277634719028\n",
      "\n",
      "English sentences 0 to 10:\n",
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "the united states is usually chilly during july , and it is usually freezing in november .\n",
      "california is usually quiet during march , and it is usually hot in june .\n",
      "the united states is sometimes mild during june , and it is cold in september .\n",
      "your least liked fruit is the grape , but my least liked is the apple .\n",
      "his favorite fruit is the orange , but my favorite is the grape .\n",
      "paris is relaxing during december , but it is usually chilly in july .\n",
      "new jersey is busy during spring , and it is never hot in march .\n",
      "our least liked fruit is the lemon , but my least liked is the grape .\n",
      "the united states is sometimes busy during january , and it is sometimes warm in november .\n",
      "\n",
      "French sentences 0 to 10:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "son fruit préféré est l'orange , mais mon préféré est le raisin .\n",
      "paris est relaxant en décembre , mais il est généralement froid en juillet .\n",
      "new jersey est occupé au printemps , et il est jamais chaude en mars .\n",
      "notre fruit est moins aimé le citron , mais mon moins aimé est le raisin .\n",
      "les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre .\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in source_text.split()})))\n",
    "\n",
    "sentences = source_text.split('\\n')\n",
    "word_counts = [len(sentence.split()) for sentence in sentences]\n",
    "print('Number of sentences: {}'.format(len(sentences)))\n",
    "print('Average number of words in a sentence: {}'.format(np.average(word_counts)))\n",
    "\n",
    "print()\n",
    "print('English sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(source_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))\n",
    "print()\n",
    "print('French sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(target_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocessing Function\n",
    "### Text to Word Ids\n",
    "As you did with other RNNs, you must turn the text into a number so the computer can understand it. In the function text_to_ids(), you'll turn source_text and target_text from words to ids. However, you need to add the <EOS> word id at the end of each sentence from target_text. This will help the neural network predict when the sentence should end.\n",
    "\n",
    "You can get the <EOS> word id by doing:\n",
    "\n",
    "    target_vocab_to_int['<EOS>']\n",
    "You can get other word ids using source_vocab_to_int and target_vocab_to_int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert source and target text to proper word ids\n",
    "    :param source_text: String that contains all the source text.\n",
    "    :param target_text: String that contains all the target text.\n",
    "    :param source_vocab_to_int: Dictionary to go from the source words to an id\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: A tuple of lists (source_id_text, target_id_text)\n",
    "    \"\"\"\n",
    "    \n",
    "    source_sentences = [sentence for sentence in source_text.split('\\n')]\n",
    "    target_sentences = [sentence + ' <EOS>' for sentence in target_text.split('\\n')]\n",
    "    \n",
    "    source_id_text = [[source_vocab_to_int[word] for word in sentence.split()] for sentence in source_sentences]\n",
    "    target_id_text = [[target_vocab_to_int[word] for word in sentence.split()] for sentence in target_sentences]\n",
    "    \n",
    "    return source_id_text, target_id_text\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_text_to_ids(text_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "helper.preprocess_and_save_data(source_path, target_path, text_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import helper\n",
    "\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Version of TensorFlow and Access to GPU\n",
    "This will check to make sure you have the correct version of TensorFlow and access to a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: No GPU found. Please use a GPU to train your neural network.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "You'll build the components necessary to build a Sequence-to-Sequence model by implementing the following functions below:\n",
    "\n",
    "model_inputs\n",
    "* process_decoding_input\n",
    "* encoding_layer\n",
    "* decoding_layer_train\n",
    "* decoding_layer_infer\n",
    "* decoding_layer\n",
    "* seq2seq_model\n",
    "\n",
    "### Input\n",
    "Implement the model_inputs() function to create TF Placeholders for the Neural Network. It should create the following placeholders:\n",
    "\n",
    "* Input text placeholder named \"input\" using the TF Placeholder name parameter with rank 2.\n",
    "* Targets placeholder with rank 2.\n",
    "* Learning rate placeholder with rank 0.\n",
    "* Keep probability placeholder named \"keep_prob\" using the TF Placeholder name parameter with rank 0.\n",
    "\n",
    "Return the placeholders in the following the tuple (Input, Targets, Learing Rate, Keep Probability)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137861"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_int_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_eng_sent = max([len(line) for line in source_int_text])\n",
    "max_len_fra_sent = max([len(line) for line in target_int_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_int_to_vocab = {v:k for k,v in target_vocab_to_int.items()}\n",
    "source_int_to_vocab = {v:k for k,v in source_vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_eng_sentences = np.zeros(shape = (len(source_int_text),max_len_eng_sent,len(source_vocab_to_int)), dtype='float32')\n",
    "tokenized_fra_sentences = np.zeros(shape = (len(target_int_text),max_len_fra_sent,len(target_vocab_to_int)), dtype='float32')\n",
    "target_data = np.zeros((len(target_int_text), max_len_fra_sent, len(target_vocab_to_int)),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137861, 17, 231)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eng_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the english and french sentences\n",
    "\n",
    "for i in range(len(target_int_text)):\n",
    "    for k,ch in enumerate(source_int_text[i]):\n",
    "        tokenized_eng_sentences[i,k,ch] = 1\n",
    "        \n",
    "    for k,ch in enumerate(target_int_text[i]):\n",
    "        tokenized_fra_sentences[i,k,ch] = 1\n",
    "\n",
    "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
    "        if k > 0:\n",
    "            target_data[i,k-1,ch] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "Implement encoding_layer() to create a Encoder RNN layer using tf.nn.dynamic_rnn()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "\n",
    "encoder_input = Input(shape=(None,len(source_vocab_to_int)))\n",
    "encoder_LSTM = LSTM(256,return_state = True)\n",
    "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
    "encoder_states = [encoder_h, encoder_c]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Training\n",
    "Create training logits using tf.contrib.seq2seq.simple_decoder_fn_train() and tf.contrib.seq2seq.dynamic_rnn_decoder(). Apply the output_fn to the tf.contrib.seq2seq.dynamic_rnn_decoder() outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder model\n",
    "\n",
    "decoder_input = Input(shape=(None,len(target_vocab_to_int)))\n",
    "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
    "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(target_vocab_to_int),activation='softmax')\n",
    "decoder_out = decoder_dense (decoder_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(358)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/50\n",
      "110288/110288 [==============================] - 289s 3ms/step - loss: 0.6684 - val_loss: 0.3432\n",
      "Epoch 2/50\n",
      "110288/110288 [==============================] - 126s 1ms/step - loss: 0.1828 - val_loss: 0.1247\n",
      "Epoch 3/50\n",
      "110288/110288 [==============================] - 125s 1ms/step - loss: 0.0432 - val_loss: 0.0295\n",
      "Epoch 4/50\n",
      "110288/110288 [==============================] - 123s 1ms/step - loss: 0.0243 - val_loss: 0.0274\n",
      "Epoch 5/50\n",
      "110288/110288 [==============================] - 118s 1ms/step - loss: 0.0173 - val_loss: 0.0227\n",
      "Epoch 6/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0137 - val_loss: 0.0145\n",
      "Epoch 7/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0113 - val_loss: 0.0134\n",
      "Epoch 8/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0097 - val_loss: 0.0119\n",
      "Epoch 9/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0082 - val_loss: 0.0120\n",
      "Epoch 10/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0072 - val_loss: 0.0097\n",
      "Epoch 11/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0064 - val_loss: 0.0105\n",
      "Epoch 12/50\n",
      "110288/110288 [==============================] - 118s 1ms/step - loss: 0.0057 - val_loss: 0.0107\n",
      "Epoch 13/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0051 - val_loss: 0.0092\n",
      "Epoch 14/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0047 - val_loss: 0.0094\n",
      "Epoch 15/50\n",
      "110288/110288 [==============================] - 118s 1ms/step - loss: 0.0042 - val_loss: 0.0100\n",
      "Epoch 16/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0038 - val_loss: 0.0092\n",
      "Epoch 17/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0036 - val_loss: 0.0096\n",
      "Epoch 18/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0033 - val_loss: 0.0092\n",
      "Epoch 19/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0030 - val_loss: 0.0090\n",
      "Epoch 20/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0028 - val_loss: 0.0091\n",
      "Epoch 21/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0026 - val_loss: 0.0088\n",
      "Epoch 22/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0024 - val_loss: 0.0093\n",
      "Epoch 23/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0023 - val_loss: 0.0091\n",
      "Epoch 24/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0022 - val_loss: 0.0090\n",
      "Epoch 25/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0020 - val_loss: 0.0092\n",
      "Epoch 26/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0019 - val_loss: 0.0091\n",
      "Epoch 27/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0019 - val_loss: 0.0091\n",
      "Epoch 28/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0018 - val_loss: 0.0094\n",
      "Epoch 29/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0017 - val_loss: 0.0097\n",
      "Epoch 30/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0017 - val_loss: 0.0093\n",
      "Epoch 31/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0016 - val_loss: 0.0099\n",
      "Epoch 32/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0015 - val_loss: 0.0100\n",
      "Epoch 33/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0015 - val_loss: 0.0097\n",
      "Epoch 34/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0014 - val_loss: 0.0098\n",
      "Epoch 35/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0014 - val_loss: 0.0103\n",
      "Epoch 36/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0014 - val_loss: 0.0101\n",
      "Epoch 37/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0013 - val_loss: 0.0104\n",
      "Epoch 38/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0013 - val_loss: 0.0107\n",
      "Epoch 39/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0012 - val_loss: 0.0105\n",
      "Epoch 40/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0012 - val_loss: 0.0100\n",
      "Epoch 41/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0012 - val_loss: 0.0106\n",
      "Epoch 42/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0012 - val_loss: 0.0106\n",
      "Epoch 43/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0011 - val_loss: 0.0105\n",
      "Epoch 44/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0011 - val_loss: 0.0111\n",
      "Epoch 45/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0012 - val_loss: 0.0108\n",
      "Epoch 46/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0011 - val_loss: 0.0110\n",
      "Epoch 47/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0011 - val_loss: 0.0110\n",
      "Epoch 48/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0010 - val_loss: 0.0111\n",
      "Epoch 49/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0010 - val_loss: 0.0115\n",
      "Epoch 50/50\n",
      "110288/110288 [==============================] - 117s 1ms/step - loss: 0.0011 - val_loss: 0.0113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd1dc0f1ac8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
    "          y=target_data,\n",
    "          batch_size=64,\n",
    "          epochs=50,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference models for testing\n",
    "\n",
    "# Encoder inference model\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
    "                                                 initial_state=decoder_input_states)\n",
    "\n",
    "decoder_states = [decoder_h , decoder_c]\n",
    "\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
    "                          outputs=[decoder_out] + decoder_states )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(inp_seq):\n",
    "    \n",
    "    # Initial states value is coming from the encoder \n",
    "    states_val = encoder_model_inf.predict(inp_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, len(target_vocab_to_int)))\n",
    "    #target_seq[0, 0, target_vocab_to_int[' ']] = 1\n",
    "    \n",
    "    translated_sent = ''\n",
    "    stop_condition = False\n",
    "    \n",
    "    while not stop_condition:\n",
    "        \n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "        \n",
    "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "        sampled_fra_char = target_int_to_vocab[max_val_index]\n",
    "        translated_sent += ' ' + sampled_fra_char\n",
    "        \n",
    "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1, 1, len(target_vocab_to_int)))\n",
    "        target_seq[0, 0, max_val_index] = 1\n",
    "        \n",
    "        states_val = [decoder_h, decoder_c]\n",
    "        \n",
    "    return translated_sent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: ['new', 'jersey', 'is', 'sometimes', 'quiet', 'during', 'autumn', ',', 'and', 'it', 'is', 'snowy', 'in', 'april', '.']\n",
      "Decoded sentence:  est parfois calme à l'automne\n",
      "-\n",
      "Input sentence: ['the', 'united', 'states', 'is', 'usually', 'chilly', 'during', 'july', ',', 'and', 'it', 'is', 'usually', 'freezing', 'in', 'november', '.']\n",
      "Decoded sentence:  est généralement froid en\n",
      "-\n",
      "Input sentence: ['california', 'is', 'usually', 'quiet', 'during', 'march', ',', 'and', 'it', 'is', 'usually', 'hot', 'in', 'june', '.']\n",
      "Decoded sentence:  déteste les calme en mars\n",
      "-\n",
      "Input sentence: ['the', 'united', 'states', 'is', 'sometimes', 'mild', 'during', 'june', ',', 'and', 'it', 'is', 'cold', 'in', 'september', '.']\n",
      "Decoded sentence:  est parfois légère en juin\n",
      "-\n",
      "Input sentence: ['your', 'least', 'liked', 'fruit', 'is', 'the', 'grape', ',', 'but', 'my', 'least', 'liked', 'is', 'the', 'apple', '.']\n",
      "Decoded sentence:  moins aimé des fruits est\n",
      "-\n",
      "Input sentence: ['his', 'favorite', 'fruit', 'is', 'the', 'orange', ',', 'but', 'my', 'favorite', 'is', 'the', 'grape', '.']\n",
      "Decoded sentence:  son fruit préféré l'orange\n",
      "-\n",
      "Input sentence: ['paris', 'is', 'relaxing', 'during', 'december', ',', 'but', 'it', 'is', 'usually', 'chilly', 'in', 'july', '.']\n",
      "Decoded sentence:  est relaxant en décembre\n",
      "-\n",
      "Input sentence: ['new', 'jersey', 'is', 'busy', 'during', 'spring', ',', 'and', 'it', 'is', 'never', 'hot', 'in', 'march', '.']\n",
      "Decoded sentence:  fruit est au printemps ,\n",
      "-\n",
      "Input sentence: ['our', 'least', 'liked', 'fruit', 'is', 'the', 'lemon', ',', 'but', 'my', 'least', 'liked', 'is', 'the', 'grape', '.']\n",
      "Decoded sentence:  fruits moins aimé est le\n",
      "-\n",
      "Input sentence: ['the', 'united', 'states', 'is', 'sometimes', 'busy', 'during', 'january', ',', 'and', 'it', 'is', 'sometimes', 'warm', 'in', 'november', '.']\n",
      "Decoded sentence:  est parfois occupé en janvier\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
    "    translated_sent = decode_seq(inp_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', [source_int_to_vocab[a] for a in source_int_text[seq_index]])\n",
    "    print('Decoded sentence:', translated_sent)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
